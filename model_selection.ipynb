{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import timedelta\n",
    "from time import time\n",
    "from os import mkdir\n",
    "from os.path import isfile, isdir\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "import importlib\n",
    "import models.convnets as convnets\n",
    "\n",
    "# Force reload of models module\n",
    "importlib.reload(convnets)\n",
    "\n",
    "# Seed for reproducability\n",
    "torch.manual_seed(123)\n",
    "torch.cuda.manual_seed(123)\n",
    "\n",
    "# Device\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to store downloaded datasets\n",
    "DOWNLOAD_DIRECTORY = \"./datasets\"\n",
    "\n",
    "# Directory to store trained models\n",
    "TRAINED_DIRECTORY = \"./models/trained\"\n",
    "\n",
    "# Ensure that trained directory exists (prevents pytorch throwing error)\n",
    "if not isdir(TRAINED_DIRECTORY):\n",
    "    mkdir(TRAINED_DIRECTORY)\n",
    "\n",
    "# Directory to store optimal, chosen models\n",
    "MODEL_DIRECTORY = \"./models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and load the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [data_exploration.ipynb](./data_exploration.ipynb), we have gathered that the given training data has a mean of ~0.1307 and a standard deviation of ~0.3081. We use this information to normalize the data on import. This feature normalization is step is important when doing machine learning, but we have to be consequent and use these normalization values for all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify transformations on load\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.1307, 0.3081)\n",
    "])\n",
    "\n",
    "dataset_train_val = datasets.MNIST(root=DOWNLOAD_DIRECTORY, train=True, download=True, transform=transform)\n",
    "dataset_test = datasets.MNIST(root=DOWNLOAD_DIRECTORY, train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and validation sets\n",
    "train_val_split = 5.0/6.0\n",
    "train_size = int(train_val_split * len(dataset_train_val))\n",
    "val_size = len(dataset_train_val) - train_size\n",
    "dataset_train, dataset_val = random_split(dataset_train_val, [train_size, val_size])\n",
    "\n",
    "batch_size = 512\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset_val, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, n_epochs, optimizer, loss_fn, train_loader, val_loader=None, device=\"cpu\"):\n",
    "    \"\"\" Training function for a model, supports addition of a validation loader to extract intermediary validation loss. \"\"\"\n",
    "    # Store intermediary training and validation losses\n",
    "    train_losses, val_losses = [], []\n",
    "    start_time = time()\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad(set_to_none = True)\n",
    "\n",
    "    if torch.cuda.is_available() and device == \"cuda\":\n",
    "        model.cuda()\n",
    "    \n",
    "    print(\" ======== Training\", model._get_name(), \"======= \")\n",
    "\n",
    "    for i in range(1, n_epochs + 1):\n",
    "        train_loss = 0\n",
    "        for imgs, labels in train_loader:\n",
    "            labels = labels.to(device=device)\n",
    "            output = model(imgs.to(device=device))\n",
    "\n",
    "            # Calculate mean batch loss and perform backward pass\n",
    "            loss = loss_fn(output, labels).mean()\n",
    "            loss.backward()\n",
    "\n",
    "            # Update parameters and zero out the gradients\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Add loss to stored training loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # If we have a validation loader, store validation loss aswell\n",
    "        val_loss = 0\n",
    "        if val_loader:\n",
    "            # Set to eval mode\n",
    "            model.eval()\n",
    "\n",
    "            # Calculate epoch's loss over validation loader\n",
    "            for imgs, labels in val_loader:\n",
    "                output = model(imgs.to(device=device))\n",
    "                loss = loss_fn(output, labels.to(device=device)).mean()\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "            # Set back to training mode\n",
    "            model.train()\n",
    "\n",
    "        # Get average epoch loss for each type\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        # Print epoch loss first 3 epochs and every 5 afterwards\n",
    "        if i <= 3 or i % 5 == 0:\n",
    "            print(timedelta(seconds=round(time() - start_time)), \"| Epoch\", i, \"| Training loss %.5f\" %train_loss, (\"| Validation loss %.5f\" %val_loss) if val_loader else \"\")\n",
    "\n",
    "        # Store this epoch's losses\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "    \n",
    "    return train_losses, val_losses if val_loader else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize and train different model architectures capable of doing image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ======== Loaded ConvNet1 ======= \n",
      "Final Training loss 0.00988 | Final Validation loss 0.06396\n",
      " ======== Loaded ConvNet2 ======= \n",
      "Final Training loss 0.00387 | Final Validation loss 0.03580\n",
      " ======== Loaded ConvNet3 ======= \n",
      "Final Training loss 0.00817 | Final Validation loss 0.04010\n",
      " ======== Loaded ConvNet1 ======= \n",
      "Final Training loss 0.01261 | Final Validation loss 0.05308\n",
      " ======== Loaded ConvNet2 ======= \n",
      "Final Training loss 0.00585 | Final Validation loss 0.03185\n",
      " ======== Loaded ConvNet3 ======= \n",
      "Final Training loss 0.00955 | Final Validation loss 0.03491\n",
      " ======== Loaded ConvNet1 ======= \n",
      "Final Training loss 0.01456 | Final Validation loss 0.06423\n",
      " ======== Loaded ConvNet2 ======= \n",
      "Final Training loss 0.00590 | Final Validation loss 0.06308\n",
      " ======== Loaded ConvNet3 ======= \n",
      "Final Training loss 0.00730 | Final Validation loss 0.06081\n",
      " ======== Loaded ConvNet1 ======= \n",
      "Final Training loss 0.01596 | Final Validation loss 0.08247\n",
      " ======== Loaded ConvNet2 ======= \n",
      "Final Training loss 0.00807 | Final Validation loss 0.04399\n",
      " ======== Loaded ConvNet3 ======= \n",
      "Final Training loss 0.00717 | Final Validation loss 0.04159\n",
      " ======== Loaded ConvNet1 ======= \n",
      "Final Training loss 0.02672 | Final Validation loss 0.06543\n",
      " ======== Loaded ConvNet2 ======= \n",
      "Final Training loss 0.02033 | Final Validation loss 0.08174\n",
      " ======== Loaded ConvNet3 ======= \n",
      "Final Training loss 0.01715 | Final Validation loss 0.07619\n",
      " ======== Loaded ConvNet1 ======= \n",
      "Final Training loss 0.03066 | Final Validation loss 0.07766\n",
      " ======== Loaded ConvNet2 ======= \n",
      "Final Training loss 0.01999 | Final Validation loss 0.05313\n",
      " ======== Loaded ConvNet3 ======= \n",
      "Final Training loss 0.02310 | Final Validation loss 0.05456\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 25\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Adam hyperparameters\n",
    "optimizer_hyperparameters = [\n",
    "    { \"lr\" : 0.001 },\n",
    "    { \"lr\" : 0.001, \"weight_decay\": 0.0001 },\n",
    "    { \"lr\" : 0.005 },\n",
    "    { \"lr\" : 0.005, \"weight_decay\": 0.0001 },\n",
    "    { \"lr\" : 0.01 },\n",
    "    { \"lr\" : 0.01, \"weight_decay\": 0.0001 },\n",
    "]\n",
    "\n",
    "# Different model architectures\n",
    "architechtures = [convnets.ConvNet1, convnets.ConvNet2, convnets.ConvNet3]\n",
    "\n",
    "# Perform model selection, storing intermediate losses\n",
    "losses = []\n",
    "models = []\n",
    "for i, hyperparameters in enumerate(optimizer_hyperparameters):\n",
    "    for architecture in architechtures:\n",
    "        # Define the model and optimizer\n",
    "        model = architecture()\n",
    "        optimizer = optim.Adam(model.parameters(), **hyperparameters)\n",
    "\n",
    "        # Path to store model under\n",
    "        model_path = TRAINED_DIRECTORY + \"/\" + model._get_name() + \"_\" + str(i+1)\n",
    "\n",
    "        # Check if the model is already saved to disk, if so load it and its loss values\n",
    "        if isfile(model_path + \".pt\"):\n",
    "            model_statedict = torch.load(model_path + \".pt\", map_location=torch.device(DEVICE))\n",
    "            model_losses = torch.load(model_path + \".loss\", map_location=torch.device(DEVICE))\n",
    "\n",
    "            model.load_state_dict(model_statedict)\n",
    "\n",
    "            losses.append((model_losses[\"train\"], model_losses[\"val\"]))\n",
    "            models.append(model)\n",
    "\n",
    "            print(\" ======== Loaded\", model._get_name(), \"======= \")\n",
    "            print(\"Final Training loss %.5f\" %model_losses[\"train\"][-1], (\"| Final Validation loss %.5f\" %model_losses[\"val\"][-1]))\n",
    "        # If not, train and save it\n",
    "        else:\n",
    "            # Train the model\n",
    "            train_losses, val_losses = train(model, n_epochs, optimizer, loss_fn, train_loader, val_loader, device=DEVICE)\n",
    "\n",
    "            # Store intermediary losses and model\n",
    "            losses.append((train_losses, val_losses))\n",
    "            models.append(model)\n",
    "\n",
    "            # Also save the model and loss values to disk\n",
    "            torch.save(model.state_dict(), model_path + \".pt\")\n",
    "            torch.save({\"train\" : train_losses, \"val\" : val_losses}, model_path + \".loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function for computing model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader, device=\"cpu\"):\n",
    "    # Store accuracy\n",
    "    total, correct = 0, 0\n",
    "\n",
    "    # Set the model to eval mode\n",
    "    model.eval()\n",
    "\n",
    "    if torch.cuda.is_available() and device == \"cuda\":\n",
    "        model.cuda()\n",
    "    \n",
    "    # Disable autograd while evaluating the model\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in data_loader:\n",
    "            labels = labels.to(device=device)\n",
    "            output = model(imgs.to(device=device))\n",
    "\n",
    "            total += len(labels)\n",
    "            correct += sum(labels == torch.argmax(output, dim=1))\n",
    "    \n",
    "    # Return the computed accuracy\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the accuracy of each model with regards to the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet1 {'lr': 0.001} had a validation accuracy of 0.986\n",
      "ConvNet2 {'lr': 0.001} had a validation accuracy of 0.991\n",
      "ConvNet3 {'lr': 0.001} had a validation accuracy of 0.988\n",
      "ConvNet1 {'lr': 0.001, 'weight_decay': 0.0001} had a validation accuracy of 0.985\n",
      "ConvNet2 {'lr': 0.001, 'weight_decay': 0.0001} had a validation accuracy of 0.991\n",
      "ConvNet3 {'lr': 0.001, 'weight_decay': 0.0001} had a validation accuracy of 0.990\n",
      "ConvNet1 {'lr': 0.005} had a validation accuracy of 0.986\n",
      "ConvNet2 {'lr': 0.005} had a validation accuracy of 0.988\n",
      "ConvNet3 {'lr': 0.005} had a validation accuracy of 0.990\n",
      "ConvNet1 {'lr': 0.005, 'weight_decay': 0.0001} had a validation accuracy of 0.979\n",
      "ConvNet2 {'lr': 0.005, 'weight_decay': 0.0001} had a validation accuracy of 0.989\n",
      "ConvNet3 {'lr': 0.005, 'weight_decay': 0.0001} had a validation accuracy of 0.989\n",
      "ConvNet1 {'lr': 0.01} had a validation accuracy of 0.986\n",
      "ConvNet2 {'lr': 0.01} had a validation accuracy of 0.986\n",
      "ConvNet3 {'lr': 0.01} had a validation accuracy of 0.984\n",
      "ConvNet1 {'lr': 0.01, 'weight_decay': 0.0001} had a validation accuracy of 0.979\n",
      "ConvNet2 {'lr': 0.01, 'weight_decay': 0.0001} had a validation accuracy of 0.988\n",
      "ConvNet3 {'lr': 0.01, 'weight_decay': 0.0001} had a validation accuracy of 0.985\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "for i, model in enumerate(models):\n",
    "    accuracy = compute_accuracy(model, val_loader, device=DEVICE)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    print(model._get_name(), optimizer_hyperparameters[i // len(architechtures)], \"had a validation accuracy of %.3f\" %accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the best performing model and estimate its performance on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best performing model was ConvNet2 {'lr': 0.001, 'weight_decay': 0.0001} with a validation accuracy of 0.991\n",
      "\n",
      "It has a test accuracy of 0.991\n"
     ]
    }
   ],
   "source": [
    "best_accuracy, best_index = torch.max(torch.tensor(accuracies), dim=0)\n",
    "selected_model = models[best_index]\n",
    "print(\"The best performing model was\", selected_model._get_name(), optimizer_hyperparameters[best_index % len(architechtures)], end=\" \")\n",
    "print(\"with a validation accuracy of %.3f\" % best_accuracy)\n",
    "\n",
    "test_accuracy = compute_accuracy(selected_model, test_loader, device=DEVICE)\n",
    "print(\"\\nIt has a test accuracy of %.3f\" %test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored selected model under './models/model.pt'\n"
     ]
    }
   ],
   "source": [
    "selected_model_path = MODEL_DIRECTORY + \"/model.pt\"\n",
    "torch.save(model.state_dict(), selected_model_path)\n",
    "print(\"Stored selected model under '\", selected_model_path, \"'\", sep=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
