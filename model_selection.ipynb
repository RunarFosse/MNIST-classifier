{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'convnets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/runarfosse/Documents/MNIST-classifier/model_selection.ipynb Cell 1\u001b[0m line \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/runarfosse/Documents/MNIST-classifier/model_selection.ipynb#W0sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mimportlib\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/runarfosse/Documents/MNIST-classifier/model_selection.ipynb#W0sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconvnets\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mconvnets\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/runarfosse/Documents/MNIST-classifier/model_selection.ipynb#W0sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mconvnets\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/runarfosse/Documents/MNIST-classifier/model_selection.ipynb#W0sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Force reload of models module\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/runarfosse/Documents/MNIST-classifier/model_selection.ipynb#W0sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m importlib\u001b[39m.\u001b[39mreload(convnets)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'convnets'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import timedelta\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "import importlib\n",
    "import models.convnets as convnets\n",
    "\n",
    "# Force reload of models module\n",
    "importlib.reload(convnets)\n",
    "\n",
    "# Seed for reproducability\n",
    "torch.manual_seed(123)\n",
    "torch.cuda.manual_seed(123)\n",
    "\n",
    "# Device\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to store downloaded datasets\n",
    "DOWNLOAD_DIRECTORY = \"./datasets\"\n",
    "\n",
    "# Directory to store trained models\n",
    "TRAINED_DIRECTORY = \"./models/trained\"\n",
    "\n",
    "# Directory to store optimal, chosen models\n",
    "MODEL_DIRECTORY = \"./models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and load the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [data_exploration.ipynb](./data_exploration.ipynb), we have gathered that the given training data has a mean of ~0.1307 and a standard deviation of ~0.3081. We use this information to normalize the data on import. This feature normalization is step is important when doing machine learning, but we have to be consequent and use these normalization values for all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify transformations on load\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.1307, 0.3081)\n",
    "])\n",
    "\n",
    "dataset_train_val = datasets.MNIST(root=DOWNLOAD_DIRECTORY, train=True, download=True, transform=transform)\n",
    "dataset_test = datasets.MNIST(root=DOWNLOAD_DIRECTORY, train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and validation sets\n",
    "train_val_split = 5.0/6.0\n",
    "train_size = int(train_val_split * len(dataset_train_val))\n",
    "val_size = len(dataset_train_val) - train_size\n",
    "dataset_train, dataset_val = random_split(dataset_train_val, [train_size, val_size])\n",
    "\n",
    "batch_size = 512\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset_val, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(dataset_val, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, n_epochs, optimizer, loss_fn, train_loader, val_loader=None, device=\"cpu\"):\n",
    "    \"\"\" Training function for a model, supports addition of a validation loader to extract intermediary validation loss. \"\"\"\n",
    "    # Store intermediary training and validation losses\n",
    "    train_losses, val_losses = [], []\n",
    "    start_time = time()\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad(set_to_none = True)\n",
    "\n",
    "    if torch.cuda.is_available() and device == \"cuda\":\n",
    "        model.cuda()\n",
    "    \n",
    "    print(\" ======== Training\", model._get_name(), \"======= \")\n",
    "\n",
    "    for i in range(1, n_epochs + 1):\n",
    "        train_loss = 0\n",
    "        for imgs, labels in train_loader:\n",
    "            output = model(imgs.to(device=device))\n",
    "\n",
    "            # Calculate mean batch loss and perform backward pass\n",
    "            loss = loss_fn(output, labels.to(device=device)).mean()\n",
    "            loss.backward()\n",
    "\n",
    "            # Update parameters and zero out the gradients\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Add loss to stored training loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # If we have a validation loader, store validation loss aswell\n",
    "        val_loss = 0\n",
    "        if val_loader:\n",
    "            # Set to eval mode\n",
    "            model.eval()\n",
    "\n",
    "            # Calculate epoch's loss over validation loader\n",
    "            for imgs, labels in val_loader:\n",
    "                output = model(imgs.to(device=device))\n",
    "                loss = loss_fn(output, labels.to(device=device)).mean()\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "            # Set back to training mode\n",
    "            model.train()\n",
    "\n",
    "        # Get average epoch loss for each type\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        # Print epoch loss first 3 epochs and every 5 afterwards\n",
    "        if i <= 3 or i % 5 == 0:\n",
    "            print(timedelta(seconds=round(time() - start_time)), \"| Epoch\", i, \"| Training loss %.5f\" %train_loss, (\"| Validation loss %.5f\" %val_loss) if val_loader else \"\")\n",
    "\n",
    "        # Store this epoch's losses\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "    \n",
    "    return train_losses, val_losses if val_loader else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize and train different model architectures capable of doing image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ======== Training ConvNet1 ======= \n",
      "0:00:36 | Epoch 1 | Training loss 0.88218 | Validation loss 0.04949\n",
      "0:01:08 | Epoch 2 | Training loss 0.18944 | Validation loss 0.03042\n",
      "0:01:38 | Epoch 3 | Training loss 0.13193 | Validation loss 0.02295\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/runarfosse/Documents/MNIST-classifier/model_selection.ipynb Cell 10\u001b[0m line \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/runarfosse/Documents/MNIST-classifier/model_selection.ipynb#W6sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhyperparameters)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/runarfosse/Documents/MNIST-classifier/model_selection.ipynb#W6sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/runarfosse/Documents/MNIST-classifier/model_selection.ipynb#W6sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m train_losses, val_losses \u001b[39m=\u001b[39m train(model, n_epochs, optimizer, loss_fn, train_loader, val_loader, device\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/runarfosse/Documents/MNIST-classifier/model_selection.ipynb#W6sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# Store intermediary losses and model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/runarfosse/Documents/MNIST-classifier/model_selection.ipynb#W6sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m losses\u001b[39m.\u001b[39mappend((train_losses, val_losses))\n",
      "\u001b[1;32m/Users/runarfosse/Documents/MNIST-classifier/model_selection.ipynb Cell 10\u001b[0m line \u001b[0;36mtrain\u001b[0;34m(model, n_epochs, optimizer, loss_fn, train_loader, val_loader, device)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/runarfosse/Documents/MNIST-classifier/model_selection.ipynb#W6sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m train_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/runarfosse/Documents/MNIST-classifier/model_selection.ipynb#W6sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m imgs, labels \u001b[39min\u001b[39;00m train_loader:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/runarfosse/Documents/MNIST-classifier/model_selection.ipynb#W6sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     output \u001b[39m=\u001b[39m model(imgs\u001b[39m.\u001b[39;49mto(device\u001b[39m=\u001b[39;49mdevice))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/runarfosse/Documents/MNIST-classifier/model_selection.ipynb#W6sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39m# Calculate mean batch loss and perform backward pass\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/runarfosse/Documents/MNIST-classifier/model_selection.ipynb#W6sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(output, labels\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice))\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/MNIST-classifier/models/convnets.py:27\u001b[0m, in \u001b[0;36mConvNet1.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool1(out)\n\u001b[1;32m     26\u001b[0m out \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv3(out))\n\u001b[0;32m---> 27\u001b[0m out \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv4(out))\n\u001b[1;32m     28\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool2(out)\n\u001b[1;32m     30\u001b[0m \u001b[39m# Fully connected part\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Adam hyperparameters\n",
    "optimizer_hyperparameters = [\n",
    "    { \"lr\" : 0.001 },\n",
    "    { \"lr\" : 0.001, \"weight_decay\": 0.0001 },\n",
    "    { \"lr\" : 0.005 },\n",
    "    { \"lr\" : 0.005, \"weight_decay\": 0.0001 },\n",
    "    { \"lr\" : 0.01 },\n",
    "    { \"lr\" : 0.01, \"weight_decay\": 0.0001 },\n",
    "]\n",
    "\n",
    "# Different model architectures\n",
    "architechtures = [convnets.ConvNet1, convnets.ConvNet2, convnets.ConvNet3]\n",
    "\n",
    "# Perform model selection, storing intermediate losses\n",
    "losses = []\n",
    "models = []\n",
    "for hyperparameters in optimizer_hyperparameters:\n",
    "    for architecture in architechtures:\n",
    "        # Define the model and optimizer\n",
    "        model = architecture()\n",
    "        optimizer = optim.Adam(model.parameters(), **hyperparameters)\n",
    "\n",
    "        # Train the model\n",
    "        train_losses, val_losses = train(model, n_epochs, optimizer, loss_fn, train_loader, val_loader, device=DEVICE)\n",
    "\n",
    "        # Store intermediary losses and model\n",
    "        losses.append((train_losses, val_losses))\n",
    "        models.append(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
